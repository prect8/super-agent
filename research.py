import asyncio
import aiohttp
import requests
from typing import Dict, List, Any, Optional
import logging
from datetime import datetime, timedelta
import re
import json
from bs4 import BeautifulSoup
import wikipedia
from duckduckgo_search import DDGS
import arxiv
import feedparser
from urllib.parse import urljoin, urlparse

# ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class FreeResearchEngine:
    """ç„¡æ–™ã§ãƒ‡ã‚£ãƒ¼ãƒ—ãƒªã‚µãƒ¼ãƒã‚’è¡Œã†ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.max_results_per_source = 5
        self.timeout = 30
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆç„¡æ–™ï¼‰
        self.news_feeds = [
            "https://rss.cnn.com/rss/edition.rss",
            "https://feeds.bbci.co.uk/news/rss.xml",
            "https://www.reuters.com/rssFeed/topNews",
            "https://rss.nytimes.com/services/xml/rss/nyt/World.xml"
        ]
    
    def extract_keywords(self, query: str) -> List[str]:
        """ã‚¯ã‚¨ãƒªã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        # åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = []
        
        # æ—¥æœ¬èªã¨è‹±èªã®å˜èªã‚’æŠ½å‡º
        words = re.findall(r'\b\w+\b', query.lower())
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å»
        stop_words = {'ã®', 'ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã§', 'ã¨', 'ã‹ã‚‰', 'ã¾ã§', 
                     'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        # å…ƒã®ã‚¯ã‚¨ãƒªã‚‚è¿½åŠ 
        keywords.insert(0, query)
        
        return keywords[:5]  # ä¸Šä½5ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    
    async def search_duckduckgo(self, query: str) -> List[Dict[str, Any]]:
        """DuckDuckGoæ¤œç´¢ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ä»˜ãï¼‰"""
        try:
            import time
            import random
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿ã®ãŸã‚ã®é…å»¶
            await asyncio.sleep(random.uniform(1, 3))
            
            ddgs = DDGS()
            results = []
            
            # ã‚¦ã‚§ãƒ–æ¤œç´¢ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
            try:
                search_results = ddgs.text(query, max_results=self.max_results_per_source)
                for result in search_results:
                    results.append({
                        "source": "DuckDuckGo Web",
                        "title": result.get("title", ""),
                        "url": result.get("href", ""),
                        "snippet": result.get("body", ""),
                        "timestamp": datetime.now().isoformat()
                    })
            except Exception as e:
                logger.warning(f"DuckDuckGo web search failed: {e}")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿ã®ãŸã‚ã®è¿½åŠ é…å»¶
            await asyncio.sleep(random.uniform(2, 4))
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
            try:
                news_results = ddgs.news(query, max_results=3)
                for result in news_results:
                    results.append({
                        "source": "DuckDuckGo News",
                        "title": result.get("title", ""),
                        "url": result.get("url", ""),
                        "snippet": result.get("body", ""),
                        "timestamp": result.get("date", datetime.now().isoformat())
                    })
            except Exception as e:
                logger.warning(f"DuckDuckGo news search failed: {e}")
            
            return results
            
        except Exception as e:
            logger.error(f"DuckDuckGo search error: {e}")
            return []
    
    async def search_wikipedia(self, query: str) -> List[Dict[str, Any]]:
        """Wikipediaæ¤œç´¢ï¼ˆç„¡æ–™ï¼‰"""
        try:
            results = []
            
            # æ—¥æœ¬èªã¨è‹±èªã§æ¤œç´¢
            for lang in ['ja', 'en']:
                wikipedia.set_lang(lang)
                
                try:
                    # æ¤œç´¢å€™è£œã‚’å–å¾—
                    search_results = wikipedia.search(query, results=3)
                    
                    for title in search_results:
                        try:
                            page = wikipedia.page(title)
                            results.append({
                                "source": f"Wikipedia ({lang})",
                                "title": page.title,
                                "url": page.url,
                                "snippet": page.summary[:500] + "...",
                                "content": page.content[:2000] + "...",
                                "timestamp": datetime.now().isoformat()
                            })
                        except wikipedia.exceptions.DisambiguationError as e:
                            # æ›–æ˜§ã•å›é¿ãƒšãƒ¼ã‚¸ã®å ´åˆã€æœ€åˆã®é¸æŠè‚¢ã‚’ä½¿ç”¨
                            try:
                                page = wikipedia.page(e.options[0])
                                results.append({
                                    "source": f"Wikipedia ({lang})",
                                    "title": page.title,
                                    "url": page.url,
                                    "snippet": page.summary[:500] + "...",
                                    "content": page.content[:2000] + "...",
                                    "timestamp": datetime.now().isoformat()
                                })
                            except:
                                continue
                        except:
                            continue
                            
                except Exception as e:
                    logger.error(f"Wikipedia search error for {lang}: {e}")
                    continue
            
            return results[:self.max_results_per_source]
            
        except Exception as e:
            logger.error(f"Wikipedia search error: {e}")
            return []
    
    async def search_arxiv(self, query: str) -> List[Dict[str, Any]]:
        """ArXivå­¦è¡“è«–æ–‡æ¤œç´¢ï¼ˆç„¡æ–™ï¼‰"""
        try:
            results = []
            
            # ArXivæ¤œç´¢
            search = arxiv.Search(
                query=query,
                max_results=3,
                sort_by=arxiv.SortCriterion.Relevance
            )
            
            for result in search.results():
                results.append({
                    "source": "ArXiv",
                    "title": result.title,
                    "url": result.entry_id,
                    "snippet": result.summary[:500] + "...",
                    "authors": [author.name for author in result.authors],
                    "published": result.published.isoformat(),
                    "timestamp": datetime.now().isoformat()
                })
            
            return results
            
        except Exception as e:
            logger.error(f"ArXiv search error: {e}")
            return []
    
    async def search_news_feeds(self, query: str) -> List[Dict[str, Any]]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹RSSãƒ•ã‚£ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆç„¡æ–™ï¼‰"""
        try:
            results = []
            keywords = self.extract_keywords(query)
            
            for feed_url in self.news_feeds:
                try:
                    feed = feedparser.parse(feed_url)
                    
                    for entry in feed.entries[:5]:  # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰5ä»¶
                        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                        title_lower = entry.title.lower()
                        summary_lower = getattr(entry, 'summary', '').lower()
                        
                        if any(keyword.lower() in title_lower or keyword.lower() in summary_lower 
                               for keyword in keywords):
                            results.append({
                                "source": f"News Feed ({urlparse(feed_url).netloc})",
                                "title": entry.title,
                                "url": entry.link,
                                "snippet": getattr(entry, 'summary', '')[:500] + "...",
                                "published": getattr(entry, 'published', ''),
                                "timestamp": datetime.now().isoformat()
                            })
                            
                except Exception as e:
                    logger.error(f"News feed error for {feed_url}: {e}")
                    continue
            
            return results[:self.max_results_per_source]
            
        except Exception as e:
            logger.error(f"News feeds search error: {e}")
            return []
    
    async def scrape_content(self, url: str) -> str:
        """Webãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ï¼ˆç„¡æ–™ï¼‰"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=self.timeout) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ä¸è¦ãªã‚¿ã‚°ã‚’é™¤å»
                        for tag in soup(['script', 'style', 'nav', 'footer', 'aside']):
                            tag.decompose()
                        
                        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
                        text = soup.get_text()
                        lines = [line.strip() for line in text.splitlines()]
                        content = ' '.join(line for line in lines if line)
                        
                        return content[:3000] + "..."  # 3000æ–‡å­—ã¾ã§
                    
        except Exception as e:
            logger.error(f"Content scraping error for {url}: {e}")
            
        return ""
    
    async def conduct_deep_research(self, query: str) -> Dict[str, Any]:
        """ãƒ‡ã‚£ãƒ¼ãƒ—ãƒªã‚µãƒ¼ãƒã®å®Ÿè¡Œ"""
        research_start = datetime.now()
        
        logger.info(f"Starting deep research for: {query}")
        
        # ä¸¦è¡Œã—ã¦è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢
        search_tasks = [
            self.search_duckduckgo(query),
            self.search_wikipedia(query),
            self.search_arxiv(query),
            self.search_news_feeds(query)
        ]
        
        search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
        
        # çµæœã‚’ãƒãƒ¼ã‚¸
        all_results = []
        for result_set in search_results:
            if isinstance(result_set, list):
                all_results.extend(result_set)
        
        # é‡è¦ãªãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è©³ç´°å–å¾—
        content_tasks = []
        for result in all_results[:5]:  # ä¸Šä½5ä»¶ã®è©³ç´°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            if result.get("url"):
                content_tasks.append(self.scrape_content(result["url"]))
        
        detailed_contents = await asyncio.gather(*content_tasks, return_exceptions=True)
        
        # è©³ç´°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒãƒ¼ã‚¸
        for i, content in enumerate(detailed_contents):
            if isinstance(content, str) and content and i < len(all_results):
                all_results[i]["detailed_content"] = content
        
        research_duration = (datetime.now() - research_start).total_seconds()
        
        return {
            "query": query,
            "total_results": len(all_results),
            "research_duration": research_duration,
            "sources_used": list(set(result["source"] for result in all_results)),
            "results": all_results,
            "timestamp": research_start.isoformat()
        }
    
    def generate_research_summary(self, research_data: Dict[str, Any]) -> str:
        """ãƒªã‚µãƒ¼ãƒçµæœã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        query = research_data["query"]
        results = research_data["results"]
        sources = research_data["sources_used"]
        
        summary = f"# ğŸ“Š '{query}' ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒªã‚µãƒ¼ãƒçµæœ\n\n"
        summary += f"**æ¤œç´¢å®Ÿè¡Œæ™‚åˆ»**: {research_data['timestamp']}\n"
        summary += f"**æ¤œç´¢æ™‚é–“**: {research_data['research_duration']:.2f}ç§’\n"
        summary += f"**æƒ…å ±æº**: {', '.join(sources)}\n"
        summary += f"**åé›†ã—ãŸæƒ…å ±æ•°**: {research_data['total_results']}ä»¶\n\n"
        
        summary += "## ğŸ” ä¸»è¦ãªç™ºè¦‹\n\n"
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ•´ç†
        by_source = {}
        for result in results:
            source = result["source"]
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(result)
        
        for source, source_results in by_source.items():
            summary += f"### {source}\n\n"
            for result in source_results[:3]:  # å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰ä¸Šä½3ä»¶
                summary += f"**{result['title']}**\n"
                summary += f"{result['snippet']}\n"
                if result.get('url'):
                    summary += f"ğŸ”— [è©³ç´°]({result['url']})\n\n"
        
        return summary

# ä½¿ç”¨ä¾‹
async def main():
    engine = FreeResearchEngine()
    query = "äººå·¥çŸ¥èƒ½ã®æœ€æ–°å‹•å‘"
    research_data = await engine.conduct_deep_research(query)
    summary = engine.generate_research_summary(research_data)
    print(summary)

if __name__ == "__main__":
    asyncio.run(main()) 